---
title: "Project 3, STAT 557"
author: "Balaji Kumar, bzk18"
date: "4/17/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Reading in the data
rm(list=ls())
setwd("/data/bzk18/Project/Misc/Spring17/STAT557/Proj3/")
t1 = Sys.time()
X = read.table("datX.txt",sep=",")
Y = (read.table("labels.txt",sep="\n"))
Y = Y[,1]
dim(X)
X = X[!duplicated(as.list(X))]
dim(X)
X = scale(log(X+0.1),center=T,scale=T)
X = data.frame(X)
t2 = Sys.time()
print(t2-t1)
```

```{r}
#Dividing training and test sets
t1 = Sys.time()
n = nrow(X)
Y = as.factor(Y)
set.seed(100)
train = sample(1:n, round(3*n/4))
X.train = X[train,]
Y.train = as.factor(Y[train])
X.test = X[-train,]
Y.test = as.factor(Y[-train])
data.train = cbind(X.train,Y.train)
data.test = cbind(X.test, Y.test)
t2 = Sys.time()
print(t2-t1)
```

```{r}
#Multicollinearity analysis
library(car)
t1 = Sys.time()
log.fit = glm(Y.train ~ . , data=data.train, family="binomial")
t2 = Sys.time()
print(t2-t1)
mcol = vif(log.fit)
t3 = Sys.time()
print(t3)
print(mcol[mcol>10])
# V114     V425     V549 
#10.04276 10.39798 10.04690 
which.max(mcol)
hist(mcol)
cat("Predictors with high variance inflation factor are ",which(mcol>10),"\n")
data.train = data.train[,-which.max(mcol)]
data.test = data.test[,-which.max(mcol)]
X = X[,-which(mcol>10)]
```

```{r}
#Dimensionality Reduction using PCA
t1=Sys.time()
pca = princomp(~ . , X)
t2 = Sys.time()
print(t2-t1)
plot(pca)
pve = 100*pca$sdev**2/sum(pca$sdev**2)
ndims = sum(cumsum(pve)<=90)
par(mfrow = c(1,1))
plot(cumsum(pve),type="o",ylab="Cumulative PVE",xlab="Principal Component",col="green")
#Since there is no clear elbow, cutoff of 90% is chosen, which gives 325 PCs
X.red = data.frame(pca$scores[,1:ndims])
X.train = X.red[train,]
X.test = X.red[-train,]
data.train = data.frame(X.train,Y.train)
data.test = data.frame(X.test, Y.test)
```

```{r}
#Feature Selection using LASSO
library(glmnet)
library(doMC)
library(pROC)
registerDoMC(cores=detectCores(T,T)-2)
set.seed(100)
t1 = Sys.time()
Lasso.fits = cv.glmnet(as.matrix(X.train),Y.train,nfolds=5,alpha=1,standardize=F,intercept=FALSE,family="binomial",parallel=T)
t2 = Sys.time()
print(t2-t1)
plot(Lasso.fits)
Lasso.lambda = Lasso.fits$lambda.min 
cat("Chosen best lambda is ",Lasso.lambda,"\n")
t1 = Sys.time()
Lasso.fit = glmnet(as.matrix(X.train),Y.train, alpha=1,standardize = F,lambda=Lasso.lambda,intercept = 0,family="binomial")
t2 = Sys.time()
print(t2-t1)
cat("Number of features discarded=",sum(Lasso.fit$beta==0),"\n")
betas = which(Lasso.fit$beta != 0)
cat("Number of predictors now are",length(betas),"\n")
hist(Lasso.fit$beta[betas])
#Training Error Rate
lasso.train = predict(Lasso.fit,type="response",newx=as.matrix(X.train))
lasso.train = as.numeric(lasso.train)
rocobj = roc(response=Y.train, predictor=lasso.train,smooth=T,auc=T)
cat("Area Under the Curve for logisitic regression+LASSO training data is ",rocobj$auc)
plot(rocobj,main="Logistic Regression+LASSO Training data")
lasso.train[lasso.train>=0.5] = 1
lasso.train[lasso.train<0.5] = 0
table(lasso.train, Y.train)
cat("Logistic Regression+LASSO Training Error Rate =",mean(lasso.train!=Y.train))
#Test Error Rate
lasso.test = predict(Lasso.fit, type="response",newx=as.matrix(X.test))
lasso.test = as.numeric(lasso.test)
rocobj = roc(response=Y.test, predictor=lasso.test,smooth=T,auc=T)
cat("Area Under the Curve for logisitic regression+LASSO testing data is ",rocobj$auc)
plot(rocobj,main="Logistic Regression+LASSO testing data")
lasso.test[lasso.test>=0.5] = 1
lasso.test[lasso.test<0.5] = 0
table(lasso.test, Y.test)
cat("Logistic Regression+LASSO testing Error Rate =",mean(lasso.test!=Y.test))
t2 = Sys.time()
print(t2-t1)
```

```{r}
#Logistic Regression
t1 = Sys.time()
set.seed(100)
log.fit = glm(Y.train ~ . ,data=data.train, family="binomial")
t2 = Sys.time()
print(t2-t1)
#Training Error
log.train = predict(log.fit,type="response")
rocobj = roc(response=Y.train, predictor=log.train,smooth=T,auc=T)
cat("Area Under the Curve for logisitic regression training data is ",rocobj$auc)
plot(rocobj,main="Logistic Regression Training data")
log.train[log.train>=0.5] = 1
log.train[log.train<0.5] = 0
table(log.train, Y.train)
cat("Logistic Regression Training Error Rate =",mean(log.train!=Y.train))
#Test Error
log.test = predict(log.fit,newdata = data.test,type="response")
rocobj=roc(response=Y.test,predictor=log.test,smooth=T,auc=T)
plot(rocobj,main="Logistic Regression Test data")
cat("Area Under the Curve for logisitic regression test data is ",rocobj$auc)
log.test[log.test<0.5] = 0
log.test[log.test!=0]=1
table(log.test,Y.test)
cat("Logistic Regression Test Error Rate = ",mean(log.test!=Y.test),"\n")
```

```{r}
#Discriminant Analysis
library(klaR)
t1 = Sys.time()
set.seed(100)
rda.fit = rda(formula=formula(log.fit),data=data.train)
t2 = Sys.time()
print(t2-t1)
cat("Regularization parameters are ",rda.fit$regularization,"\n")
cat("RDA cross-validated Training Error Rate = ",rda.fit$error.rate[2],"\n")
#Training Error Rate
rda.train = predict(rda.fit, type="response")
rda.train = as.numeric(rda.train$posterior[,"1"])
rocobj = roc(response=Y.train, predictor=rda.train,smooth=T,auc=T)
plot(rocobj,main="RDA training data")
cat("AUC for RDA training data is",rocobj$auc,"\n")
rda.train[rda.train<0.5]=0
rda.train[rda.train!=0]=1
table(rda.train,Y.train)
cat("RDA training Error Rate is ",mean(rda.train!=Y.train),"\n")
#Test Error Rate
rda.test = predict(rda.fit,newdata=data.test,type="response")
rda.test = as.numeric(rda.test$posterior[,"1"])
rocobj=roc(response=Y.test,predictor=rda.test,smooth=T,auc=T)
cat("AUC for RDA test data is ",rocobj$auc,"\n")
plot(rocobj,main="RDA test data")
rda.test[rda.test<0.5]=0
rda.test[rda.test!=0]=1
table(rda.test,Y.test)
cat("RDA test error rate = ",mean(rda.test!=Y.test),"\n")
```

